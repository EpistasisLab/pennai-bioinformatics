{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a table showing the hyperparameters for each method in the PLMB mock experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>seed</th>\n",
       "      <th>parameters</th>\n",
       "      <th>bal_accuracy</th>\n",
       "      <th>test_bal_accuracy</th>\n",
       "      <th>parameters_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agaricus-lepiota</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1393</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.984774</td>\n",
       "      <td>0.986054</td>\n",
       "      <td>8398128649823667144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agaricus-lepiota</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1393</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8713062024257713210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agaricus-lepiota</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1393</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.915847</td>\n",
       "      <td>0.908704</td>\n",
       "      <td>3716052301044598032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agaricus-lepiota</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1393</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.978882</td>\n",
       "      <td>0.980518</td>\n",
       "      <td>1418614510876412902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agaricus-lepiota</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1393</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>-3215202352506167200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset           algorithm  seed  \\\n",
       "0  agaricus-lepiota  AdaBoostClassifier  1393   \n",
       "1  agaricus-lepiota  AdaBoostClassifier  1393   \n",
       "2  agaricus-lepiota  AdaBoostClassifier  1393   \n",
       "3  agaricus-lepiota  AdaBoostClassifier  1393   \n",
       "4  agaricus-lepiota  AdaBoostClassifier  1393   \n",
       "\n",
       "                                          parameters  bal_accuracy  \\\n",
       "0  {'algorithm': 'SAMME.R', 'base_estimator': Non...      0.984774   \n",
       "1  {'algorithm': 'SAMME.R', 'base_estimator': Non...      1.000000   \n",
       "2  {'algorithm': 'SAMME.R', 'base_estimator': Non...      0.915847   \n",
       "3  {'algorithm': 'SAMME.R', 'base_estimator': Non...      0.978882   \n",
       "4  {'algorithm': 'SAMME.R', 'base_estimator': Non...      0.999672   \n",
       "\n",
       "   test_bal_accuracy      parameters_hash  \n",
       "0           0.986054  8398128649823667144  \n",
       "1           1.000000 -8713062024257713210  \n",
       "2           0.908704  3716052301044598032  \n",
       "3           0.980518  1418614510876412902  \n",
       "4           0.999483 -3215202352506167200  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = '/media/bill/Drive/projects/pennai/analysis/notebooks/pmlb_sklearn-benchmark2-data-mock_experiment.tsv.gz'\n",
    "df = pd.read_csv(file,sep='\\t',compression='gzip')\n",
    "df['parameters'] = df['parameters'].apply(lambda x: eval(x))\n",
    "df['parameters_hash'] = df['parameters'].apply(lambda x: hash(frozenset(x.items())))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier()\n",
      "hyper_params = {\n",
      " 'algorithm': ['SAMME.R'] ,\n",
      " 'base_estimator': [None] ,\n",
      " 'learning_rate': [0.1, 1.0, 0.5, 100.0, 10.0, 0.01, 50.0] ,\n",
      " 'n_estimators': [100, 1000, 10, 50, 500] ,\n",
      " 'random_state': [1393] ,\n",
      "}\n",
      "BernoulliNB()\n",
      "hyper_params = {\n",
      " 'alpha': [0.75, 0.5, 0.0, 0.25, 0.1, 5.0, 1.0, 10.0, 50.0, 25.0] ,\n",
      " 'binarize': [0.1, 1.0, 0.9, 0.75, 0.0, 0.5, 0.25] ,\n",
      " 'class_prior': [None] ,\n",
      " 'fit_prior': ['false', 'true'] ,\n",
      "}\n",
      "DecisionTreeClassifier()\n",
      "hyper_params = {\n",
      " 'class_weight': [None] ,\n",
      " 'criterion': ['entropy', 'gini'] ,\n",
      " 'max_depth': [None] ,\n",
      " 'max_features': [0.1, 0.25, 0.75, 0.5, 'log2', None, 'sqrt'] ,\n",
      " 'max_leaf_nodes': [None] ,\n",
      " 'min_impurity_decrease': [0.0] ,\n",
      " 'min_impurity_split': [None] ,\n",
      " 'min_samples_leaf': [1] ,\n",
      " 'min_samples_split': [2] ,\n",
      " 'min_weight_fraction_leaf': [0.25, 0.5, 0.3, 0.45, 0.0, 0.05, 0.2, 0.35, 0.4, 0.1, 0.15] ,\n",
      " 'presort': [0] ,\n",
      " 'random_state': [1393] ,\n",
      " 'splitter': ['best'] ,\n",
      "}\n",
      "ExtraTreesClassifier()\n",
      "hyper_params = {\n",
      " 'bootstrap': [0] ,\n",
      " 'class_weight': [None] ,\n",
      " 'criterion': ['entropy', 'gini'] ,\n",
      " 'max_depth': [None] ,\n",
      " 'max_features': [0.1, 0.25, 0.5, 0.75, 'log2', None, 'sqrt'] ,\n",
      " 'max_leaf_nodes': [None] ,\n",
      " 'min_impurity_decrease': [0.0] ,\n",
      " 'min_impurity_split': [None] ,\n",
      " 'min_samples_leaf': [1] ,\n",
      " 'min_samples_split': [2] ,\n",
      " 'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.45, 0.05, 0.15, 0.25, 0.35, 0.3, 0.5, 0.4] ,\n",
      " 'n_estimators': [100, 1000, 10, 50, 500] ,\n",
      " 'n_jobs': [None] ,\n",
      " 'oob_score': [0] ,\n",
      " 'random_state': [1393] ,\n",
      " 'verbose': [0] ,\n",
      " 'warm_start': [0] ,\n",
      "}\n",
      "GradientBoostingClassifier()\n",
      "hyper_params = {\n",
      " 'criterion': ['friedman_mse'] ,\n",
      " 'init': [None] ,\n",
      " 'learning_rate': [0.1, 0.5, 1.0, 10.0, 0.01] ,\n",
      " 'loss': ['deviance'] ,\n",
      " 'max_depth': [1, 2, 3, 4, 5, 10, None, 50, 20] ,\n",
      " 'max_features': ['sqrt', 'log2', None] ,\n",
      " 'max_leaf_nodes': [None] ,\n",
      " 'min_impurity_decrease': [0.0] ,\n",
      " 'min_impurity_split': [None] ,\n",
      " 'min_samples_leaf': [1] ,\n",
      " 'min_samples_split': [2] ,\n",
      " 'min_weight_fraction_leaf': [0.0] ,\n",
      " 'n_estimators': [100, 1000, 10, 50, 500] ,\n",
      " 'n_iter_no_change': [None] ,\n",
      " 'presort': ['auto'] ,\n",
      " 'random_state': [1393] ,\n",
      " 'subsample': [1.0] ,\n",
      " 'tol': [0.0] ,\n",
      " 'validation_fraction': [0.1] ,\n",
      " 'verbose': [0] ,\n",
      " 'warm_start': [0] ,\n",
      "}\n",
      "KNeighborsClassifier()\n",
      "hyper_params = {\n",
      " 'algorithm': ['auto'] ,\n",
      " 'leaf_size': [30] ,\n",
      " 'metric': ['minkowski'] ,\n",
      " 'metric_params': [None] ,\n",
      " 'n_jobs': [None] ,\n",
      " 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] ,\n",
      " 'p': [2] ,\n",
      " 'weights': ['distance', 'uniform'] ,\n",
      "}\n",
      "LogisticRegression()\n",
      "hyper_params = {\n",
      " 'C': [0.359, 0.0, 0.046, 2.783, 166.81, 1291.55, 10000.0, 0.006, 21.544, 0.001] ,\n",
      " 'class_weight': [None] ,\n",
      " 'dual': [0, 1] ,\n",
      " 'fit_intercept': [0, 1] ,\n",
      " 'intercept_scaling': [1] ,\n",
      " 'l1_ratio': [None] ,\n",
      " 'max_iter': [100] ,\n",
      " 'multi_class': ['warn'] ,\n",
      " 'n_jobs': [None] ,\n",
      " 'penalty': ['l1', 'l2'] ,\n",
      " 'random_state': [1393] ,\n",
      " 'solver': ['warn'] ,\n",
      " 'tol': [0.0] ,\n",
      " 'verbose': [0] ,\n",
      " 'warm_start': [0] ,\n",
      "}\n",
      "RandomForestClassifier()\n",
      "hyper_params = {\n",
      " 'bootstrap': [1] ,\n",
      " 'class_weight': [None] ,\n",
      " 'criterion': ['entropy', 'gini'] ,\n",
      " 'max_depth': [None] ,\n",
      " 'max_features': [0.25, 0.75, 0.5, 0.1, 'log2', None, 'sqrt'] ,\n",
      " 'max_leaf_nodes': [None] ,\n",
      " 'min_impurity_decrease': [0.0] ,\n",
      " 'min_impurity_split': [None] ,\n",
      " 'min_samples_leaf': [1] ,\n",
      " 'min_samples_split': [2] ,\n",
      " 'min_weight_fraction_leaf': [0.45, 0.1, 0.3, 0.05, 0.15, 0.0, 0.5, 0.4, 0.2, 0.35, 0.25] ,\n",
      " 'n_estimators': [100, 1000, 10, 50, 500] ,\n",
      " 'n_jobs': [None] ,\n",
      " 'oob_score': [0] ,\n",
      " 'random_state': [1393] ,\n",
      " 'verbose': [0] ,\n",
      " 'warm_start': [0] ,\n",
      "}\n",
      "SGDClassifier()\n",
      "hyper_params = {\n",
      " 'alpha': [0.0, 0.01, 0.001] ,\n",
      " 'average': [0] ,\n",
      " 'class_weight': [None] ,\n",
      " 'early_stopping': [0] ,\n",
      " 'epsilon': [0.1] ,\n",
      " 'eta0': [0.1, 1.0, 0.01] ,\n",
      " 'fit_intercept': [0, 1] ,\n",
      " 'l1_ratio': [0.0, 1.0, 0.75, 0.25, 0.5] ,\n",
      " 'learning_rate': ['invscaling', 'constant'] ,\n",
      " 'loss': ['squared_hinge', 'modified_huber', 'hinge', 'perceptron', 'log'] ,\n",
      " 'max_iter': [1000] ,\n",
      " 'n_iter_no_change': [5] ,\n",
      " 'n_jobs': [None] ,\n",
      " 'penalty': ['elasticnet'] ,\n",
      " 'power_t': [0.5, 1.0, 0.1, 0.0, 100.0, 10.0] ,\n",
      " 'random_state': [1393] ,\n",
      " 'shuffle': [1] ,\n",
      " 'tol': [0.001] ,\n",
      " 'validation_fraction': [0.1] ,\n",
      " 'verbose': [0] ,\n",
      " 'warm_start': [0] ,\n",
      "}\n",
      "SVC()\n",
      "hyper_params = {\n",
      " 'C': [0.1, 1, 0.01] ,\n",
      " 'cache_size': [200] ,\n",
      " 'class_weight': [None] ,\n",
      " 'coef0': [0.5, 0.0, 1.0] ,\n",
      " 'decision_function_shape': ['ovr'] ,\n",
      " 'degree': [2, 3] ,\n",
      " 'gamma': ['auto', 'scale'] ,\n",
      " 'kernel': ['rbf', 'poly'] ,\n",
      " 'max_iter': [5000] ,\n",
      " 'probability': [0] ,\n",
      " 'random_state': [1393] ,\n",
      " 'shrinking': [1] ,\n",
      " 'tol': [0.001] ,\n",
      " 'verbose': [0] ,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "alg_hp = {}\n",
    "for alg, dfa in df.groupby('algorithm'):\n",
    "    if alg not in alg_hp.keys():\n",
    "        alg_hp[alg] = {}\n",
    "#     hp = dfa.parameters.values[0].keys()\n",
    "    hp = {}\n",
    "    for _,dfap in dfa.groupby('parameters_hash'):\n",
    "        params = dfap.parameters.values[0]\n",
    "        for k,v in params.items():\n",
    "            if k not in hp.keys():\n",
    "                hp[k] = set()\n",
    "            try: v = round(v,3)\n",
    "            except: pass\n",
    "            hp[k].update([v])\n",
    "    alg_hp[alg] = hp\n",
    "    print(alg,'()',sep='')\n",
    "    print('hyper_params = {')\n",
    "    for k,v in alg_hp[alg].items():\n",
    "        print('',\"'\"+k+\"':\",list(v),',')\n",
    "    print('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make pretty latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\toprule\n",
      "\\multirow{5}{*}{AdaBoostClassifier}\t&\t algorithm \t&\t ['SAMME.R'] \\\\\n",
      "\t&\t base\\_estimator \t&\t [None] \\\\\n",
      "\t&\t learning\\_rate \t&\t [0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0] \\\\\n",
      "\t&\t n\\_estimators \t&\t [10, 50, 100, 500, 1000] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\\midrule\n",
      "\\multirow{4}{*}{BernoulliNB}\t&\t alpha \t&\t [0.0, 0.1, 0.25, 0.5, 0.75, 1.0, 5.0, 10.0, 25.0, 50.0] \\\\\n",
      "\t&\t binarize \t&\t [0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0] \\\\\n",
      "\t&\t class\\_prior \t&\t [None] \\\\\n",
      "\t&\t fit\\_prior \t&\t ['false', 'true'] \\\\\n",
      "\\midrule\n",
      "\\multirow{13}{*}{DecisionTreeClassifier}\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t criterion \t&\t ['entropy', 'gini'] \\\\\n",
      "\t&\t max\\_depth \t&\t [None] \\\\\n",
      "\t&\t max\\_features \t&\t [0.1, 0.25, 0.5, 0.75, 'log2', None, 'sqrt'] \\\\\n",
      "\t&\t max\\_leaf\\_nodes \t&\t [None] \\\\\n",
      "\t&\t min\\_impurity\\_decrease \t&\t [0.0] \\\\\n",
      "\t&\t min\\_impurity\\_split \t&\t [None] \\\\\n",
      "\t&\t min\\_samples\\_leaf \t&\t [1] \\\\\n",
      "\t&\t min\\_samples\\_split \t&\t [2] \\\\\n",
      "\t&\t min\\_weight\\_fraction\\_leaf \t&\t [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5] \\\\\n",
      "\t&\t presort \t&\t [0] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t splitter \t&\t ['best'] \\\\\n",
      "\\midrule\n",
      "\\multirow{17}{*}{ExtraTreesClassifier}\t&\t bootstrap \t&\t [0] \\\\\n",
      "\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t criterion \t&\t ['entropy', 'gini'] \\\\\n",
      "\t&\t max\\_depth \t&\t [None] \\\\\n",
      "\t&\t max\\_features \t&\t [0.1, 0.25, 0.5, 0.75, 'log2', None, 'sqrt'] \\\\\n",
      "\t&\t max\\_leaf\\_nodes \t&\t [None] \\\\\n",
      "\t&\t min\\_impurity\\_decrease \t&\t [0.0] \\\\\n",
      "\t&\t min\\_impurity\\_split \t&\t [None] \\\\\n",
      "\t&\t min\\_samples\\_leaf \t&\t [1] \\\\\n",
      "\t&\t min\\_samples\\_split \t&\t [2] \\\\\n",
      "\t&\t min\\_weight\\_fraction\\_leaf \t&\t [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5] \\\\\n",
      "\t&\t n\\_estimators \t&\t [10, 50, 100, 500, 1000] \\\\\n",
      "\t&\t n\\_jobs \t&\t [None] \\\\\n",
      "\t&\t oob\\_score \t&\t [0] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n",
      "\t&\t warm\\_start \t&\t [0] \\\\\n",
      "\\midrule\n",
      "\\multirow{21}{*}{GradientBoostingClassifier}\t&\t criterion \t&\t ['friedman\\_mse'] \\\\\n",
      "\t&\t init \t&\t [None] \\\\\n",
      "\t&\t learning\\_rate \t&\t [0.01, 0.1, 0.5, 1.0, 10.0] \\\\\n",
      "\t&\t loss \t&\t ['deviance'] \\\\\n",
      "\t&\t max\\_depth \t&\t [1, 2, 3, 4, 5, 10, 20, 50, None] \\\\\n",
      "\t&\t max\\_features \t&\t ['sqrt', 'log2', None] \\\\\n",
      "\t&\t max\\_leaf\\_nodes \t&\t [None] \\\\\n",
      "\t&\t min\\_impurity\\_decrease \t&\t [0.0] \\\\\n",
      "\t&\t min\\_impurity\\_split \t&\t [None] \\\\\n",
      "\t&\t min\\_samples\\_leaf \t&\t [1] \\\\\n",
      "\t&\t min\\_samples\\_split \t&\t [2] \\\\\n",
      "\t&\t min\\_weight\\_fraction\\_leaf \t&\t [0.0] \\\\\n",
      "\t&\t n\\_estimators \t&\t [10, 50, 100, 500, 1000] \\\\\n",
      "\t&\t n\\_iter\\_no\\_change \t&\t [None] \\\\\n",
      "\t&\t presort \t&\t ['auto'] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t subsample \t&\t [1.0] \\\\\n",
      "\t&\t tol \t&\t [0.0] \\\\\n",
      "\t&\t validation\\_fraction \t&\t [0.1] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n",
      "\t&\t warm\\_start \t&\t [0] \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{KNeighborsClassifier}\t&\t algorithm \t&\t ['auto'] \\\\\n",
      "\t&\t leaf\\_size \t&\t [30] \\\\\n",
      "\t&\t metric \t&\t ['minkowski'] \\\\\n",
      "\t&\t metric\\_params \t&\t [None] \\\\\n",
      "\t&\t n\\_jobs \t&\t [None] \\\\\n",
      "\t&\t n\\_neighbors \t&\t [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \\\\\n",
      "\t&\t p \t&\t [2] \\\\\n",
      "\t&\t weights \t&\t ['distance', 'uniform'] \\\\\n",
      "\\midrule\n",
      "\\multirow{15}{*}{LogisticRegression}\t&\t C \t&\t [0.0, 0.001, 0.006, 0.046, 0.359, 2.783, 21.544, 166.81, 1291.55, 10000.0] \\\\\n",
      "\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t dual \t&\t [0, 1] \\\\\n",
      "\t&\t fit\\_intercept \t&\t [0, 1] \\\\\n",
      "\t&\t intercept\\_scaling \t&\t [1] \\\\\n",
      "\t&\t l1\\_ratio \t&\t [None] \\\\\n",
      "\t&\t max\\_iter \t&\t [100] \\\\\n",
      "\t&\t multi\\_class \t&\t ['warn'] \\\\\n",
      "\t&\t n\\_jobs \t&\t [None] \\\\\n",
      "\t&\t penalty \t&\t ['l1', 'l2'] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t solver \t&\t ['warn'] \\\\\n",
      "\t&\t tol \t&\t [0.0] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n",
      "\t&\t warm\\_start \t&\t [0] \\\\\n",
      "\\midrule\n",
      "\\multirow{17}{*}{RandomForestClassifier}\t&\t bootstrap \t&\t [1] \\\\\n",
      "\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t criterion \t&\t ['entropy', 'gini'] \\\\\n",
      "\t&\t max\\_depth \t&\t [None] \\\\\n",
      "\t&\t max\\_features \t&\t [0.1, 0.25, 0.5, 0.75, 'log2', None, 'sqrt'] \\\\\n",
      "\t&\t max\\_leaf\\_nodes \t&\t [None] \\\\\n",
      "\t&\t min\\_impurity\\_decrease \t&\t [0.0] \\\\\n",
      "\t&\t min\\_impurity\\_split \t&\t [None] \\\\\n",
      "\t&\t min\\_samples\\_leaf \t&\t [1] \\\\\n",
      "\t&\t min\\_samples\\_split \t&\t [2] \\\\\n",
      "\t&\t min\\_weight\\_fraction\\_leaf \t&\t [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5] \\\\\n",
      "\t&\t n\\_estimators \t&\t [10, 50, 100, 500, 1000] \\\\\n",
      "\t&\t n\\_jobs \t&\t [None] \\\\\n",
      "\t&\t oob\\_score \t&\t [0] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n",
      "\t&\t warm\\_start \t&\t [0] \\\\\n",
      "\\midrule\n",
      "\\multirow{21}{*}{SGDClassifier}\t&\t alpha \t&\t [0.0, 0.001, 0.01] \\\\\n",
      "\t&\t average \t&\t [0] \\\\\n",
      "\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t early\\_stopping \t&\t [0] \\\\\n",
      "\t&\t epsilon \t&\t [0.1] \\\\\n",
      "\t&\t eta0 \t&\t [0.01, 0.1, 1.0] \\\\\n",
      "\t&\t fit\\_intercept \t&\t [0, 1] \\\\\n",
      "\t&\t l1\\_ratio \t&\t [0.0, 0.25, 0.5, 0.75, 1.0] \\\\\n",
      "\t&\t learning\\_rate \t&\t ['invscaling', 'constant'] \\\\\n",
      "\t&\t loss \t&\t ['squared\\_hinge', 'modified\\_huber', 'hinge', 'perceptron', 'log'] \\\\\n",
      "\t&\t max\\_iter \t&\t [1000] \\\\\n",
      "\t&\t n\\_iter\\_no\\_change \t&\t [5] \\\\\n",
      "\t&\t n\\_jobs \t&\t [None] \\\\\n",
      "\t&\t penalty \t&\t ['elasticnet'] \\\\\n",
      "\t&\t power\\_t \t&\t [0.0, 0.1, 0.5, 1.0, 10.0, 100.0] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t shuffle \t&\t [1] \\\\\n",
      "\t&\t tol \t&\t [0.001] \\\\\n",
      "\t&\t validation\\_fraction \t&\t [0.1] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n",
      "\t&\t warm\\_start \t&\t [0] \\\\\n",
      "\\midrule\n",
      "\\multirow{14}{*}{SVC}\t&\t C \t&\t [0.01, 0.1, 1] \\\\\n",
      "\t&\t cache\\_size \t&\t [200] \\\\\n",
      "\t&\t class\\_weight \t&\t [None] \\\\\n",
      "\t&\t coef0 \t&\t [0.0, 0.5, 1.0] \\\\\n",
      "\t&\t decision\\_function\\_shape \t&\t ['ovr'] \\\\\n",
      "\t&\t degree \t&\t [2, 3] \\\\\n",
      "\t&\t gamma \t&\t ['auto', 'scale'] \\\\\n",
      "\t&\t kernel \t&\t ['rbf', 'poly'] \\\\\n",
      "\t&\t max\\_iter \t&\t [5000] \\\\\n",
      "\t&\t probability \t&\t [0] \\\\\n",
      "\t&\t random\\_state \t&\t [1393] \\\\\n",
      "\t&\t shrinking \t&\t [1] \\\\\n",
      "\t&\t tol \t&\t [0.001] \\\\\n",
      "\t&\t verbose \t&\t [0] \\\\\n"
     ]
    }
   ],
   "source": [
    "sep = '\\t&\\t'\n",
    "first=True\n",
    "for k,v in alg_hp.items():\n",
    "    if first: \n",
    "        print('\\\\toprule')\n",
    "    np = len(v.keys())\n",
    "    for i,(ak,av) in enumerate(v.items()):\n",
    "        if i == 0: \n",
    "            if not first:\n",
    "                print('\\midrule')\n",
    "            print('\\\\multirow{{{NP}}}{{*}}{{{ALG}}}'.format(NP=np,ALG=k),end='')\n",
    "        p = str(ak).replace('_','\\\\_')\n",
    "        vals = list(av)\n",
    "#         if not any([type(v) in [str,type(None)] for v in vals]):\n",
    "#             vals = sorted(vals)\n",
    "#         else:\n",
    "        fvals = [v for v in vals if type(v) in [int,float]]\n",
    "        ovals = [v for v in vals if type(v) not in [int,float]]\n",
    "        vals = sorted(fvals) + ovals\n",
    "        vals = str(vals).replace('{','\\\\{').replace('}','\\\\}').replace('_','\\\\_')\n",
    "        print(sep,p,sep,vals,'\\\\\\\\')\n",
    "    first = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
